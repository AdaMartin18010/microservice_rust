# 云原生部署完整流程

> 基于Rust 1.90和最新依赖版本的云原生微服务部署指南

## 📋 概述

本文档提供了完整的云原生部署流程，包括容器化、Kubernetes编排、服务网格集成、监控告警等全栈解决方案。

## 🎯 部署架构

### 1. 整体架构图

```mermaid
graph TB
    subgraph "用户层"
        U[用户] --> LB[负载均衡器]
    end
    
    subgraph "网关层"
        LB --> GW[API网关]
        GW --> ISTIO[Istio Service Mesh]
    end
    
    subgraph "应用层"
        ISTIO --> MS1[微服务1 - Axum]
        ISTIO --> MS2[微服务2 - Poem]
        ISTIO --> MS3[微服务3 - Volo RPC]
    end
    
    subgraph "数据层"
        MS1 --> DB[(PostgreSQL)]
        MS2 --> CACHE[(Redis)]
        MS3 --> MQ[消息队列]
    end
    
    subgraph "监控层"
        MS1 --> PROM[Prometheus]
        MS2 --> JAEGER[Jaeger]
        MS3 --> GRAFANA[Grafana]
    end
```

### 2. 技术栈选择

| 组件 | 技术选择 | 版本 | 理由 |
|------|----------|------|------|
| 容器运行时 | Docker | 24.0+ | 标准化容器化 |
| 编排平台 | Kubernetes | 1.28+ | 云原生标准 |
| 服务网格 | Istio | 1.19+ | 流量管理、安全 |
| API网关 | Istio Gateway | 1.19+ | 统一入口 |
| 监控 | Prometheus + Grafana | 最新 | 指标监控 |
| 追踪 | Jaeger | 最新 | 分布式追踪 |
| 日志 | ELK Stack | 最新 | 日志聚合 |

## 🐳 容器化

### 1. Dockerfile优化

#### 1.1 多阶段构建

```dockerfile
# Dockerfile
# 构建阶段
FROM rust:1.90-slim as builder

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /app

# 复制Cargo文件
COPY Cargo.toml Cargo.lock ./

# 构建依赖（利用Docker缓存）
RUN cargo build --release --locked

# 复制源代码
COPY src ./src
COPY examples ./examples
COPY benches ./benches

# 重新构建（只编译源代码变更）
RUN touch src/main.rs && cargo build --release --locked

# 运行阶段
FROM debian:bookworm-slim

# 安装运行时依赖
RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    libpq5 \
    && rm -rf /var/lib/apt/lists/*

# 创建非root用户
RUN groupadd -r appuser && useradd -r -g appuser appuser

# 设置工作目录
WORKDIR /app

# 复制二进制文件
COPY --from=builder /app/target/release/microservice-server /app/
COPY --from=builder /app/target/release/microservice-client /app/

# 设置权限
RUN chown -R appuser:appuser /app
USER appuser

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# 暴露端口
EXPOSE 8080

# 启动命令
CMD ["./microservice-server"]
```

#### 1.2 安全优化

```dockerfile
# Dockerfile.security
FROM rust:1.90-slim as builder

# 使用非root用户构建
RUN groupadd -r builder && useradd -r -g builder builder
USER builder

# 设置安全的环境变量
ENV RUSTFLAGS="-C target-cpu=native -C opt-level=3"
ENV CARGO_INCREMENTAL=0
ENV CARGO_PROFILE_RELEASE_LTO=true
ENV CARGO_PROFILE_RELEASE_CODEGEN_UNITS=1

# 构建配置
WORKDIR /home/builder/app
COPY --chown=builder:builder Cargo.toml Cargo.lock ./
RUN cargo build --release --locked

COPY --chown=builder:builder src ./src
RUN touch src/main.rs && cargo build --release --locked

# 运行阶段
FROM gcr.io/distroless/cc-debian12

# 复制二进制文件
COPY --from=builder /home/builder/app/target/release/microservice-server /app/

# 使用非特权用户
USER 65534:65534

# 暴露端口
EXPOSE 8080

# 启动命令
ENTRYPOINT ["/app/microservice-server"]
```

### 2. 构建脚本

```bash
#!/bin/bash
# scripts/build_docker.sh

set -e

# 配置
IMAGE_NAME="microservice"
VERSION="${1:-latest}"
REGISTRY="${2:-your-registry.com}"

echo "🐳 开始构建Docker镜像..."

# 1. 构建镜像
echo "构建镜像: $REGISTRY/$IMAGE_NAME:$VERSION"
docker build -t "$REGISTRY/$IMAGE_NAME:$VERSION" .

# 2. 安全扫描
echo "执行安全扫描..."
if command -v trivy &> /dev/null; then
    trivy image "$REGISTRY/$IMAGE_NAME:$VERSION"
else
    echo "Trivy未安装，跳过安全扫描"
fi

# 3. 镜像优化
echo "优化镜像大小..."
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
    wagoodman/dive:latest "$REGISTRY/$IMAGE_NAME:$VERSION"

# 4. 推送到仓库
echo "推送镜像到仓库..."
docker push "$REGISTRY/$IMAGE_NAME:$VERSION"

echo "✅ Docker镜像构建完成"
```

## ☸️ Kubernetes部署

### 1. 命名空间配置

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: microservice
  labels:
    name: microservice
    istio-injection: enabled
```

### 2. 配置管理

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: microservice-config
  namespace: microservice
data:
  config.toml: |
    [server]
    host = "0.0.0.0"
    port = 8080
    
    [database]
    url = "postgresql://user:password@postgres:5432/microservice"
    
    [redis]
    url = "redis://redis:6379"
    
    [logging]
    level = "info"
    format = "json"
    
    [metrics]
    enabled = true
    port = 9090
```

```yaml
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: microservice-secrets
  namespace: microservice
type: Opaque
data:
  database-password: <base64-encoded-password>
  jwt-secret: <base64-encoded-jwt-secret>
  api-key: <base64-encoded-api-key>
```

### 3. 数据库部署

```yaml
# k8s/postgres.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: microservice
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: microservice
        - name: POSTGRES_USER
          value: user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: microservice-secrets
              key: database-password
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: microservice
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: microservice
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

### 4. Redis部署

```yaml
# k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: microservice
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: microservice
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
```

### 5. 微服务部署

```yaml
# k8s/microservice.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice
  namespace: microservice
spec:
  replicas: 3
  selector:
    matchLabels:
      app: microservice
  template:
    metadata:
      labels:
        app: microservice
        version: v1
    spec:
      containers:
      - name: microservice
        image: your-registry.com/microservice:latest
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: RUST_LOG
          value: "info"
        - name: CONFIG_PATH
          value: "/app/config/config.toml"
        volumeMounts:
        - name: config
          mountPath: /app/config
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: microservice-config
---
apiVersion: v1
kind: Service
metadata:
  name: microservice
  namespace: microservice
spec:
  selector:
    app: microservice
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090
```

## 🌐 服务网格集成

### 1. Istio安装

```bash
#!/bin/bash
# scripts/install_istio.sh

set -e

echo "🌐 安装Istio服务网格..."

# 1. 下载Istio
curl -L https://istio.io/downloadIstio | sh -
cd istio-*

# 2. 安装Istio
./bin/istioctl install --set values.defaultRevision=default -y

# 3. 启用sidecar自动注入
kubectl label namespace microservice istio-injection=enabled

# 4. 验证安装
./bin/istioctl verify-install

echo "✅ Istio安装完成"
```

### 2. 网关配置

```yaml
# k8s/gateway.yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: microservice-gateway
  namespace: microservice
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - microservice.example.com
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: microservice-tls
    hosts:
    - microservice.example.com
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: microservice-vs
  namespace: microservice
spec:
  hosts:
  - microservice.example.com
  gateways:
  - microservice-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1
    route:
    - destination:
        host: microservice
        port:
          number: 80
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
```

### 3. 流量管理

```yaml
# k8s/traffic-management.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: microservice-dr
  namespace: microservice
spec:
  host: microservice
  trafficPolicy:
    loadBalancer:
      simple: LEAST_CONN
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: microservice-canary
  namespace: microservice
spec:
  hosts:
  - microservice
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: microservice
        subset: v2
        port:
          number: 80
      weight: 100
  - route:
    - destination:
        host: microservice
        subset: v1
        port:
          number: 80
      weight: 90
    - destination:
        host: microservice
        subset: v2
        port:
          number: 80
      weight: 10
```

## 📊 监控与告警

### 1. Prometheus配置

```yaml
# k8s/prometheus.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
    - "alert_rules.yml"
    
    scrape_configs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: config
        configMap:
          name: prometheus-config
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  selector:
    app: prometheus
  ports:
  - port: 9090
    targetPort: 9090
```

### 2. Grafana配置

```yaml
# k8s/grafana.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  selector:
    app: grafana
  ports:
  - port: 3000
    targetPort: 3000
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-pvc
  namespace: monitoring
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

### 3. 告警规则

```yaml
# k8s/alert-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-rules
  namespace: monitoring
data:
  alert_rules.yml: |
    groups:
    - name: microservice
      rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"
      
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }} seconds"
      
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.pod }} is restarting frequently"
```

## 🔄 CI/CD流水线

### 1. GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy to Kubernetes

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        components: rustfmt, clippy
    
    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Run tests
      run: cargo test --all-features
    
    - name: Run clippy
      run: cargo clippy --all-targets --all-features -- -D warnings
    
    - name: Check formatting
      run: cargo fmt --all -- --check

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG }}
    
    - name: Deploy to Kubernetes
      run: |
        # 更新镜像标签
        sed -i "s|image: .*|image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}|g" k8s/microservice.yaml
        
        # 应用配置
        kubectl apply -f k8s/namespace.yaml
        kubectl apply -f k8s/configmap.yaml
        kubectl apply -f k8s/secret.yaml
        kubectl apply -f k8s/postgres.yaml
        kubectl apply -f k8s/redis.yaml
        kubectl apply -f k8s/microservice.yaml
        
        # 等待部署完成
        kubectl rollout status deployment/microservice -n microservice --timeout=300s
```

### 2. ArgoCD配置

```yaml
# argocd/application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: microservice
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/microservice
    targetRevision: HEAD
    path: k8s
  destination:
    server: https://kubernetes.default.svc
    namespace: microservice
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - PrunePropagationPolicy=foreground
    - PruneLast=true
```

## 🛡️ 安全配置

### 1. 网络策略

```yaml
# k8s/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: microservice-netpol
  namespace: microservice
spec:
  podSelector:
    matchLabels:
      app: microservice
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - podSelector:
        matchLabels:
          app: microservice
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
```

### 2. Pod安全策略

```yaml
# k8s/pod-security-policy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: microservice-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - 'configMap'
  - 'emptyDir'
  - 'projected'
  - 'secret'
  - 'downwardAPI'
  - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

## 📈 性能优化

### 1. 资源限制

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: microservice-hpa
  namespace: microservice
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: microservice
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### 2. 垂直Pod自动扩缩容

```yaml
# k8s/vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: microservice-vpa
  namespace: microservice
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: microservice
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: microservice
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 1000m
        memory: 1Gi
```

## 🔧 运维工具

### 1. 部署脚本

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

ENVIRONMENT="${1:-development}"
NAMESPACE="microservice"

echo "🚀 开始部署到 $ENVIRONMENT 环境..."

# 1. 检查kubectl连接
kubectl cluster-info

# 2. 应用基础配置
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml

# 3. 部署数据库
kubectl apply -f k8s/postgres.yaml
kubectl apply -f k8s/redis.yaml

# 4. 等待数据库就绪
kubectl wait --for=condition=ready pod -l app=postgres -n $NAMESPACE --timeout=300s
kubectl wait --for=condition=ready pod -l app=redis -n $NAMESPACE --timeout=300s

# 5. 部署应用
kubectl apply -f k8s/microservice.yaml

# 6. 等待应用就绪
kubectl wait --for=condition=ready pod -l app=microservice -n $NAMESPACE --timeout=300s

# 7. 应用服务网格配置
kubectl apply -f k8s/gateway.yaml
kubectl apply -f k8s/traffic-management.yaml

echo "✅ 部署完成"
```

### 2. 回滚脚本

```bash
#!/bin/bash
# scripts/rollback.sh

set -e

NAMESPACE="microservice"
REVISION="${1:-previous}"

echo "🔄 开始回滚到 $REVISION 版本..."

# 回滚部署
kubectl rollout undo deployment/microservice -n $NAMESPACE --to-revision=$REVISION

# 等待回滚完成
kubectl rollout status deployment/microservice -n $NAMESPACE --timeout=300s

echo "✅ 回滚完成"
```

## 📚 最佳实践总结

### 1. 部署原则

- **渐进式部署**: 使用蓝绿部署或金丝雀发布
- **自动化**: 实现完全自动化的CI/CD流水线
- **监控**: 建立完善的监控和告警体系
- **安全**: 实施多层安全防护

### 2. 性能优化

- **资源管理**: 合理配置资源请求和限制
- **自动扩缩容**: 使用HPA和VPA实现弹性伸缩
- **缓存策略**: 合理使用Redis缓存
- **连接池**: 优化数据库连接池配置

### 3. 运维管理

- **日志聚合**: 使用ELK Stack收集和分析日志
- **指标监控**: 使用Prometheus + Grafana监控
- **分布式追踪**: 使用Jaeger进行链路追踪
- **健康检查**: 实现完善的健康检查机制

### 4. 安全措施

- **网络隔离**: 使用NetworkPolicy限制网络访问
- **Pod安全**: 使用PodSecurityPolicy限制权限
- **密钥管理**: 使用Kubernetes Secret管理敏感信息
- **镜像安全**: 定期扫描镜像安全漏洞

## 🔗 相关资源

- [Kubernetes官方文档](https://kubernetes.io/docs/)
- [Istio官方文档](https://istio.io/latest/docs/)
- [Prometheus官方文档](https://prometheus.io/docs/)
- [Grafana官方文档](https://grafana.com/docs/)
- [多框架集成最佳实践](./多框架集成最佳实践.md)

---

**注意**: 本文档基于2025年9月的最新技术栈，建议定期更新以保持时效性。
