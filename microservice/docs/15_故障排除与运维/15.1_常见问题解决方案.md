# 15.1 常见问题解决方案

> 微服务架构常见问题的诊断和解决方案

## 📋 目录

- [服务启动问题](#服务启动问题)
- [性能问题](#性能问题)
- [数据一致性问题](#数据一致性问题)
- [网络连接问题](#网络连接问题)
- [监控告警问题](#监控告警问题)
- [安全相关问题](#安全相关问题)

## 服务启动问题

### 1. 容器启动失败

**问题症状：**
- Pod状态为CrashLoopBackOff
- 容器不断重启
- 日志显示启动错误

**诊断步骤：**
```bash
# 检查Pod状态
kubectl get pods -n production -l app=microservice

# 查看Pod详细信息
kubectl describe pod <pod-name> -n production

# 查看容器日志
kubectl logs <pod-name> -n production --previous

# 检查事件
kubectl get events -n production --sort-by='.lastTimestamp'
```

**常见原因和解决方案：**

| 原因 | 症状 | 解决方案 |
|------|------|----------|
| 配置错误 | 环境变量缺失 | 检查ConfigMap和Secret配置 |
| 资源不足 | OOMKilled | 增加内存限制或优化内存使用 |
| 依赖服务不可用 | 连接超时 | 检查依赖服务状态和网络连接 |
| 权限问题 | 权限被拒绝 | 检查ServiceAccount和RBAC配置 |

### 2. 数据库连接失败

**问题症状：**
- 应用无法连接到数据库
- 连接池耗尽
- 连接超时

**诊断脚本：**
```bash
#!/bin/bash
# 数据库连接诊断脚本

echo "🔍 检查数据库连接..."

# 检查数据库Pod状态
kubectl get pods -n production -l app=postgres

# 检查数据库服务
kubectl get svc -n production -l app=postgres

# 测试数据库连接
kubectl exec -n production <app-pod> -- nc -zv postgres-service 5432

# 检查数据库日志
kubectl logs -n production -l app=postgres --tail=50

# 检查连接数
kubectl exec -n production <postgres-pod> -- psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"
```

**解决方案：**
```yaml
# 数据库连接池配置优化
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-config
data:
  DATABASE_MAX_CONNECTIONS: "50"
  DATABASE_MIN_CONNECTIONS: "10"
  DATABASE_CONNECTION_TIMEOUT: "30s"
  DATABASE_IDLE_TIMEOUT: "600s"
```

## 性能问题

### 1. 高延迟问题

**问题症状：**
- 响应时间超过预期
- 用户投诉应用缓慢
- 监控显示P95延迟过高

**诊断工具：**
```bash
#!/bin/bash
# 性能诊断脚本

echo "🔍 开始性能诊断..."

# 检查CPU使用率
kubectl top pods -n production

# 检查内存使用率
kubectl top pods -n production --containers

# 检查网络延迟
kubectl exec -n production <pod-name> -- ping -c 5 postgres-service

# 检查数据库性能
kubectl exec -n production <postgres-pod> -- psql -U postgres -c "
SELECT query, mean_time, calls, total_time 
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;"
```

**优化方案：**
```rust
// 性能优化代码示例
use tokio::time::{timeout, Duration};

pub struct OptimizedService {
    db_pool: sqlx::PgPool,
    cache: redis::Client,
}

impl OptimizedService {
    pub async fn get_user_optimized(&self, user_id: Uuid) -> Result<User, Box<dyn std::error::Error>> {
        // 1. 先尝试从缓存获取
        if let Some(user) = self.get_cached_user(user_id).await? {
            return Ok(user);
        }

        // 2. 从数据库获取，设置超时
        let user = timeout(
            Duration::from_secs(5),
            self.get_user_from_db(user_id)
        ).await??;

        // 3. 缓存结果
        self.cache_user(&user).await?;
        
        Ok(user)
    }

    async fn get_user_from_db(&self, user_id: Uuid) -> Result<User, sqlx::Error> {
        sqlx::query_as::<_, User>("SELECT * FROM users WHERE id = $1")
            .bind(user_id)
            .fetch_one(&self.db_pool)
            .await
    }
}
```

### 2. 内存泄漏问题

**问题症状：**
- 内存使用持续增长
- 频繁的OOMKilled
- 垃圾回收频繁

**诊断方法：**
```bash
# 检查内存使用趋势
kubectl top pods -n production --containers

# 检查内存限制
kubectl describe pod <pod-name> -n production | grep -A 5 "Limits:"

# 分析内存使用
kubectl exec -n production <pod-name> -- cat /proc/meminfo
```

**解决方案：**
```rust
// 内存优化代码示例
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct MemoryOptimizedService {
    // 使用Arc减少内存复制
    cache: Arc<RwLock<HashMap<String, Arc<String>>>>,
    // 定期清理过期数据
    cleanup_interval: Duration,
}

impl MemoryOptimizedService {
    pub fn new() -> Self {
        let service = Self {
            cache: Arc::new(RwLock::new(HashMap::new())),
            cleanup_interval: Duration::from_secs(300), // 5分钟清理一次
        };
        
        // 启动清理任务
        service.start_cleanup_task();
        service
    }

    fn start_cleanup_task(&self) {
        let cache = self.cache.clone();
        let interval = self.cleanup_interval;
        
        tokio::spawn(async move {
            let mut cleanup_timer = tokio::time::interval(interval);
            loop {
                cleanup_timer.tick().await;
                
                // 清理过期数据
                let mut cache_guard = cache.write().await;
                cache_guard.retain(|_, value| {
                    // 检查是否过期
                    !value.is_empty()
                });
            }
        });
    }
}
```

## 数据一致性问题

### 1. 分布式事务问题

**问题症状：**
- 数据不一致
- 事务回滚失败
- 部分更新成功

**诊断脚本：**
```bash
#!/bin/bash
# 数据一致性检查脚本

echo "🔍 检查数据一致性..."

# 检查事务日志
kubectl logs -n production -l app=microservice | grep -i "transaction\|rollback\|commit"

# 检查数据库状态
kubectl exec -n production <postgres-pod> -- psql -U postgres -c "
SELECT * FROM pg_stat_database WHERE datname = 'microservice';"

# 检查死锁
kubectl exec -n production <postgres-pod> -- psql -U postgres -c "
SELECT * FROM pg_stat_activity WHERE state = 'active';"
```

**解决方案：**
```rust
// Saga模式实现
pub struct SagaManager {
    steps: Vec<SagaStep>,
    compensations: Vec<CompensationStep>,
}

impl SagaManager {
    pub async fn execute_saga(&self) -> Result<(), SagaError> {
        let mut executed_steps = Vec::new();
        
        for (index, step) in self.steps.iter().enumerate() {
            match step.execute().await {
                Ok(_) => {
                    executed_steps.push(index);
                }
                Err(e) => {
                    // 执行补偿操作
                    self.compensate(&executed_steps).await?;
                    return Err(SagaError::ExecutionFailed(e));
                }
            }
        }
        
        Ok(())
    }

    async fn compensate(&self, executed_steps: &[usize]) -> Result<(), SagaError> {
        // 逆序执行补偿操作
        for &step_index in executed_steps.iter().rev() {
            if let Some(compensation) = self.compensations.get(step_index) {
                if let Err(e) = compensation.execute().await {
                    return Err(SagaError::CompensationFailed(e));
                }
            }
        }
        Ok(())
    }
}
```

### 2. 缓存一致性问题

**问题症状：**
- 缓存数据过期
- 数据不同步
- 读取到旧数据

**诊断方法：**
```bash
# 检查Redis状态
kubectl exec -n production <redis-pod> -- redis-cli info memory

# 检查缓存命中率
kubectl exec -n production <redis-pod> -- redis-cli info stats | grep hit_rate

# 检查缓存键
kubectl exec -n production <redis-pod> -- redis-cli keys "*user*"
```

**解决方案：**
```rust
// 缓存一致性解决方案
pub struct ConsistentCache {
    redis: redis::Client,
    db: sqlx::PgPool,
}

impl ConsistentCache {
    pub async fn get_user(&self, user_id: Uuid) -> Result<Option<User>, Box<dyn std::error::Error>> {
        // 1. 尝试从缓存获取
        if let Some(user) = self.get_from_cache(user_id).await? {
            return Ok(Some(user));
        }

        // 2. 从数据库获取
        let user = self.get_from_db(user_id).await?;
        
        // 3. 写入缓存
        if let Some(ref user) = user {
            self.set_cache(user_id, user).await?;
        }

        Ok(user)
    }

    pub async fn update_user(&self, user: &User) -> Result<(), Box<dyn std::error::Error>> {
        // 1. 更新数据库
        self.update_db(user).await?;
        
        // 2. 更新缓存
        self.set_cache(user.id, user).await?;
        
        Ok(())
    }

    pub async fn delete_user(&self, user_id: Uuid) -> Result<(), Box<dyn std::error::Error>> {
        // 1. 删除数据库记录
        self.delete_from_db(user_id).await?;
        
        // 2. 删除缓存
        self.delete_from_cache(user_id).await?;
        
        Ok(())
    }
}
```

## 网络连接问题

### 1. 服务间通信失败

**问题症状：**
- 服务调用超时
- 连接被拒绝
- 网络不可达

**诊断脚本：**
```bash
#!/bin/bash
# 网络连接诊断脚本

echo "🔍 检查网络连接..."

# 检查服务状态
kubectl get svc -n production

# 检查端点
kubectl get endpoints -n production

# 测试服务间连接
kubectl exec -n production <pod-name> -- nc -zv user-service 8080
kubectl exec -n production <pod-name> -- nc -zv order-service 8080

# 检查DNS解析
kubectl exec -n production <pod-name> -- nslookup user-service

# 检查网络策略
kubectl get networkpolicies -n production
```

**解决方案：**
```yaml
# 网络策略配置
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: microservice-network-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: microservice
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: nginx-ingress
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
```

### 2. 负载均衡问题

**问题症状：**
- 请求分布不均
- 部分实例过载
- 响应时间差异大

**诊断方法：**
```bash
# 检查负载均衡器状态
kubectl get svc -n production -o wide

# 检查Pod分布
kubectl get pods -n production -o wide

# 检查HPA状态
kubectl get hpa -n production

# 检查资源使用
kubectl top pods -n production
```

**解决方案：**
```yaml
# 负载均衡配置优化
apiVersion: v1
kind: Service
metadata:
  name: microservice-service
  namespace: production
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  selector:
    app: microservice
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
  type: LoadBalancer
  sessionAffinity: None
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: microservice-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: microservice
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## 监控告警问题

### 1. 指标收集失败

**问题症状：**
- Prometheus无法抓取指标
- 监控面板显示无数据
- 告警规则不触发

**诊断脚本：**
```bash
#!/bin/bash
# 监控诊断脚本

echo "🔍 检查监控状态..."

# 检查Prometheus状态
kubectl get pods -n monitoring -l app=prometheus

# 检查指标端点
kubectl exec -n production <pod-name> -- curl -s http://localhost:9090/metrics | head -20

# 检查服务发现
kubectl port-forward -n monitoring svc/prometheus 9090:9090 &
sleep 5
curl -s "http://localhost:9090/api/v1/targets" | jq '.data.activeTargets[] | select(.health != "up")'

# 检查告警规则
curl -s "http://localhost:9090/api/v1/rules" | jq '.data.groups[].rules[] | select(.state != "ok")'
```

**解决方案：**
```yaml
# 监控配置优化
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
      - job_name: 'microservice'
        static_configs:
          - targets: ['microservice-service:9090']
        metrics_path: /metrics
        scrape_interval: 10s
        scrape_timeout: 5s
        honor_labels: true
        relabel_configs:
          - source_labels: [__address__]
            target_label: instance
            regex: '([^:]+):.*'
            replacement: '${1}'
```

### 2. 告警规则问题

**问题症状：**
- 告警不触发
- 误报警告
- 告警延迟

**诊断方法：**
```bash
# 检查告警规则状态
kubectl port-forward -n monitoring svc/prometheus 9090:9090 &
curl -s "http://localhost:9090/api/v1/rules" | jq '.data.groups[].rules[]'

# 检查告警历史
kubectl logs -n monitoring -l app=alertmanager --tail=100

# 测试告警规则
curl -s "http://localhost:9090/api/v1/query?query=up" | jq
```

**解决方案：**
```yaml
# 告警规则优化
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-rules
  namespace: monitoring
data:
  alert_rules.yml: |
    groups:
    - name: microservice
      rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for 5 minutes"
          runbook_url: "https://wiki.company.com/runbooks/high-error-rate"
      
      - alert: ServiceDown
        expr: up{job="microservice"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "Microservice is not responding"
          runbook_url: "https://wiki.company.com/runbooks/service-down"
```

## 安全相关问题

### 1. 认证失败

**问题症状：**
- JWT令牌无效
- 用户无法登录
- 权限验证失败

**诊断脚本：**
```bash
#!/bin/bash
# 认证问题诊断脚本

echo "🔍 检查认证状态..."

# 检查JWT密钥
kubectl get secret jwt-secret -n production -o yaml

# 检查认证服务日志
kubectl logs -n production -l app=auth-service --tail=50

# 测试JWT令牌
kubectl exec -n production <pod-name> -- curl -H "Authorization: Bearer <token>" http://localhost:8080/api/users/me
```

**解决方案：**
```rust
// JWT认证优化
use jsonwebtoken::{decode, encode, Header, Algorithm, Validation, DecodingKey, EncodingKey};
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String,
    pub exp: usize,
    pub iat: usize,
    pub role: String,
}

pub struct JwtService {
    encoding_key: EncodingKey,
    decoding_key: DecodingKey,
    validation: Validation,
}

impl JwtService {
    pub fn new(secret: &str) -> Self {
        let encoding_key = EncodingKey::from_secret(secret.as_ref());
        let decoding_key = DecodingKey::from_secret(secret.as_ref());
        let mut validation = Validation::new(Algorithm::HS256);
        validation.leeway = 60; // 允许60秒的时间偏差
        
        Self {
            encoding_key,
            decoding_key,
            validation,
        }
    }

    pub fn generate_token(&self, user_id: String, role: String) -> Result<String, jsonwebtoken::errors::Error> {
        let now = chrono::Utc::now().timestamp() as usize;
        let claims = Claims {
            sub: user_id,
            exp: now + 3600, // 1小时过期
            iat: now,
            role,
        };

        encode(&Header::default(), &claims, &self.encoding_key)
    }

    pub fn validate_token(&self, token: &str) -> Result<Claims, jsonwebtoken::errors::Error> {
        let token_data = decode::<Claims>(token, &self.decoding_key, &self.validation)?;
        Ok(token_data.claims)
    }
}
```

### 2. 权限问题

**问题症状：**
- 访问被拒绝
- 权限不足
- RBAC配置错误

**诊断方法：**
```bash
# 检查RBAC配置
kubectl get roles -n production
kubectl get rolebindings -n production
kubectl get serviceaccounts -n production

# 检查权限
kubectl auth can-i get pods --as=system:serviceaccount:production:microservice -n production

# 检查审计日志
kubectl get events -n production --sort-by='.lastTimestamp' | grep -i "forbidden\|unauthorized"
```

**解决方案：**
```yaml
# RBAC配置优化
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: microservice-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: microservice-rolebinding
  namespace: production
subjects:
- kind: ServiceAccount
  name: microservice
  namespace: production
roleRef:
  kind: Role
  name: microservice-role
  apiGroup: rbac.authorization.k8s.io
```

## 总结

本故障排除指南提供了微服务架构常见问题的诊断和解决方案：

1. **服务启动问题**：容器启动失败、数据库连接问题
2. **性能问题**：高延迟、内存泄漏
3. **数据一致性问题**：分布式事务、缓存一致性
4. **网络连接问题**：服务间通信、负载均衡
5. **监控告警问题**：指标收集、告警规则
6. **安全相关问题**：认证失败、权限问题

通过系统化的诊断方法和针对性的解决方案，可以快速定位和解决微服务架构中的各种问题，确保系统的稳定运行。
