//! é«˜çº§ AI/ML åŠŸèƒ½æ¼”ç¤º
//!
//! æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é«˜çº§ AI/ML åŠŸèƒ½ï¼š
//! - å¤šæ¨¡æ€ AI æœåŠ¡ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ï¼‰
//! - æ™ºèƒ½æ¨èç³»ç»Ÿ
//! - å¼‚å¸¸æ£€æµ‹å’Œç›‘æ§
//! - æ¨¡å‹ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶
//! - å®æ—¶æ¨ç†æœåŠ¡

use anyhow::Result;

// å¯¼å…¥æˆ‘ä»¬çš„é«˜çº§ AI/ML æ¨¡å—
#[cfg(feature = "with-ai-ml")]
use microservice::ai_ml_advanced::*;

/// AI/ML æ¼”ç¤ºç®¡ç†å™¨
#[cfg(feature = "with-ai-ml")]
pub struct AIMLDemoManager {
    service: AdvancedAIMLService,
}

/// AI/ML æ¼”ç¤ºç®¡ç†å™¨ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
#[cfg(not(feature = "with-ai-ml"))]
pub struct AIMLDemoManager {
    // ç©ºç»“æ„ä½“ï¼Œç”¨äºé AI/ML ç‰¹æ€§ç‰ˆæœ¬
}

impl AIMLDemoManager {
    #[cfg(feature = "with-ai-ml")]
    pub fn new() -> Self {
        let config = AIMLConfig {
            model_cache_size: 20,
            inference_timeout_ms: 10000,
            batch_size: 64,
            enable_gpu: false,
            model_update_interval: 7200, // 2 hours
        };

        let service = AdvancedAIMLService::new(config);

        Self { service }
    }

    #[cfg(not(feature = "with-ai-ml"))]
    pub fn new() -> Self {
        Self {
            // ç©ºç»“æ„ä½“ï¼Œç”¨äºé AI/ML ç‰¹æ€§ç‰ˆæœ¬
        }
    }

    /// åˆå§‹åŒ– AI/ML æœåŠ¡
    #[cfg(feature = "with-ai-ml")]
    pub async fn initialize(&self) -> Result<()> {
        println!("ğŸ¤– åˆå§‹åŒ– AI/ML æœåŠ¡...");

        // æ³¨å†Œæ¨¡æ‹Ÿæ–‡æœ¬æ¨¡å‹
        let text_model = AIMLServiceFactory::create_mock_text_model("bert-base", "1.0.0");
        self.service
            .register_text_model("bert-base".to_string(), text_model)
            .await?;

        println!("âœ… æ–‡æœ¬æ¨¡å‹å·²æ³¨å†Œ");

        // è¿™é‡Œå¯ä»¥æ³¨å†Œæ›´å¤šæ¨¡å‹
        // let image_model = AIMLServiceFactory::create_mock_image_model("resnet50", "1.0.0");
        // self.service.register_image_model("resnet50".to_string(), image_model).await?;

        println!("âœ… AI/ML æœåŠ¡åˆå§‹åŒ–å®Œæˆ");
        Ok(())
    }

    /// åˆå§‹åŒ– AI/ML æœåŠ¡ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn initialize(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–");
        Ok(())
    }

    /// æ¼”ç¤ºæ–‡æœ¬åˆ†æåŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_text_analysis(&self) -> Result<()> {
        println!("\nğŸ“ æ¼”ç¤ºæ–‡æœ¬åˆ†æåŠŸèƒ½:");
        println!("================================");

        let test_texts = vec![
            "è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„äº§å“ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼",
            "è¿™ä¸ªæœåŠ¡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸æ¨èä½¿ç”¨ã€‚",
            "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€‚",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚",
        ];

        for (i, text) in test_texts.iter().enumerate() {
            println!("\næ–‡æœ¬ {}: {}", i + 1, text);

            // æƒ…æ„Ÿåˆ†æ
            let request = AIMLRequest::TextAnalysis {
                text: text.clone(),
                task: TextTask::SentimentAnalysis,
            };

            let response = self.service.process_request(request).await?;
            if let AIMLResult::TextAnalysis { sentiment, .. } = response.result {
                if let Some(sentiment) = sentiment {
                    println!(
                        "  ğŸ’­ æƒ…æ„Ÿåˆ†æ: {} (ç½®ä¿¡åº¦: {:.2})",
                        sentiment.sentiment, sentiment.score
                    );
                }
            }

            // å…³é”®è¯æå–
            let request = AIMLRequest::TextAnalysis {
                text: text.clone(),
                task: TextTask::KeywordExtraction,
            };

            let response = self.service.process_request(request).await?;
            if let AIMLResult::TextAnalysis { keywords, .. } = response.result {
                if let Some(keywords) = keywords {
                    println!("  ğŸ”‘ å…³é”®è¯: {:?}", keywords);
                }
            }

            // æ–‡æœ¬æ‘˜è¦
            let request = AIMLRequest::TextAnalysis {
                text: text.clone(),
                task: TextTask::Summarization,
            };

            let response = self.service.process_request(request).await?;
            if let AIMLResult::TextAnalysis { summary, .. } = response.result {
                if let Some(summary) = summary {
                    println!("  ğŸ“„ æ‘˜è¦: {}", summary);
                }
            }

            println!("  â±ï¸  å¤„ç†æ—¶é—´: {}ms", response.processing_time_ms);
        }

        Ok(())
    }

    /// æ¼”ç¤ºæ–‡æœ¬åˆ†æåŠŸèƒ½ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_text_analysis(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ–‡æœ¬åˆ†ææ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºå›¾åƒåˆ†æåŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_image_analysis(&self) -> Result<()> {
        println!("\nğŸ–¼ï¸  æ¼”ç¤ºå›¾åƒåˆ†æåŠŸèƒ½:");
        println!("================================");

        // æ¨¡æ‹Ÿå›¾åƒæ•°æ®
        let image_data = vec![0u8; 1024]; // æ¨¡æ‹Ÿ 1KB å›¾åƒæ•°æ®

        let image_tasks = vec![
            ("å›¾åƒåˆ†ç±»", ImageTask::Classification),
            ("ç›®æ ‡æ£€æµ‹", ImageTask::ObjectDetection),
            ("äººè„¸æ£€æµ‹", ImageTask::FaceDetection),
            ("æ–‡æœ¬æå–", ImageTask::TextExtraction),
            ("åœºæ™¯åˆ†æ", ImageTask::SceneAnalysis),
        ];

        for (task_name, task) in image_tasks {
            println!("\n{}:", task_name);

            let request = AIMLRequest::ImageAnalysis {
                image_data: image_data.clone(),
                task,
            };

            let response = self.service.process_request(request).await?;

            match response.result {
                AIMLResult::ImageAnalysis {
                    classification,
                    objects,
                    faces,
                    extracted_text,
                    scene,
                    ..
                } => {
                    if let Some(classification) = classification {
                        println!("  åˆ†ç±»ç»“æœ: {:?}", classification);
                    }
                    if let Some(objects) = objects {
                        println!("  æ£€æµ‹åˆ°çš„å¯¹è±¡: {:?}", objects);
                    }
                    if let Some(faces) = faces {
                        println!("  æ£€æµ‹åˆ°çš„äººè„¸: {:?}", faces);
                    }
                    if let Some(text) = extracted_text {
                        println!("  æå–çš„æ–‡æœ¬: {}", text);
                    }
                    if let Some(scene) = scene {
                        println!("  åœºæ™¯åˆ†æ: {}", scene);
                    }
                }
                _ => println!("  æœªæ”¯æŒçš„ç»“æœç±»å‹"),
            }

            println!("  â±ï¸  å¤„ç†æ—¶é—´: {}ms", response.processing_time_ms);
        }

        Ok(())
    }

    /// æ¼”ç¤ºå›¾åƒåˆ†æåŠŸèƒ½ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_image_analysis(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡å›¾åƒåˆ†ææ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºæ¨èç³»ç»ŸåŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_recommendation_system(&self) -> Result<()> {
        println!("\nğŸ¯ æ¼”ç¤ºæ¨èç³»ç»ŸåŠŸèƒ½:");
        println!("================================");

        let test_users = vec!["user1", "user2", "user3"];
        let item_types = vec!["movie", "book", "product", "music"];

        for user_id in test_users {
            println!("\nç”¨æˆ· {} çš„æ¨è:", user_id);

            for item_type in &item_types {
                let request = AIMLRequest::Recommendation {
                    user_id: user_id.to_string(),
                    item_type: item_type.to_string(),
                    limit: 5,
                };

                let response = self.service.process_request(request).await?;

                if let AIMLResult::Recommendation { items, reasoning } = response.result {
                    println!("  {} æ¨è:", item_type);
                    for (i, item) in items.iter().enumerate() {
                        println!(
                            "    {}. {} (åˆ†æ•°: {:.2}) - {}",
                            i + 1,
                            item.item_id,
                            item.score,
                            item.reason
                        );
                    }
                    println!("  æ¨èç†ç”±: {}", reasoning);
                }

                println!("  â±ï¸  å¤„ç†æ—¶é—´: {}ms", response.processing_time_ms);
            }
        }

        Ok(())
    }

    /// æ¼”ç¤ºæ¨èç³»ç»ŸåŠŸèƒ½ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_recommendation_system(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ¨èç³»ç»Ÿæ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_anomaly_detection(&self) -> Result<()> {
        println!("\nğŸš¨ æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½:");
        println!("================================");

        let test_datasets = vec![
            ("æ­£å¸¸æ•°æ®", vec![1.0, 2.0, 3.0, 2.5, 2.8, 3.2, 2.9, 3.1]),
            ("å¼‚å¸¸æ•°æ®", vec![1.0, 2.0, 3.0, 2.5, 2.8, 10.0, 2.9, 3.1]), // åŒ…å«å¼‚å¸¸å€¼
            ("ç³»ç»ŸæŒ‡æ ‡", vec![0.5, 0.6, 0.7, 0.8, 0.9, 0.85, 0.75, 0.65]),
            (
                "ç½‘ç»œå»¶è¿Ÿ",
                vec![10.0, 12.0, 15.0, 11.0, 13.0, 100.0, 14.0, 12.5],
            ), // åŒ…å«ç½‘ç»œå¼‚å¸¸
        ];

        for (dataset_name, data) in test_datasets {
            println!("\n{}: {:?}", dataset_name, data);

            let request = AIMLRequest::AnomalyDetection {
                data: data.clone(),
                threshold: 2.0, // æ ‡å‡†å·®é˜ˆå€¼
            };

            let response = self.service.process_request(request).await?;

            if let AIMLResult::AnomalyDetection {
                is_anomaly,
                anomaly_score,
                explanation,
            } = response.result
            {
                let status = if is_anomaly {
                    "ğŸš¨ å¼‚å¸¸"
                } else {
                    "âœ… æ­£å¸¸"
                };
                println!("  {} å¼‚å¸¸åˆ†æ•°: {:.3}", status, anomaly_score);
                println!("  è§£é‡Š: {}", explanation);
            }

            println!("  â±ï¸  å¤„ç†æ—¶é—´: {}ms", response.processing_time_ms);
        }

        Ok(())
    }

    /// æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_anomaly_detection(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡å¼‚å¸¸æ£€æµ‹æ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºæ‰¹é‡å¤„ç†
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_batch_processing(&self) -> Result<()> {
        println!("\nğŸ“¦ æ¼”ç¤ºæ‰¹é‡å¤„ç†:");
        println!("================================");

        let batch_size = 10;
        let requests: Vec<AIMLRequest> = (1..=batch_size)
            .map(|i| AIMLRequest::TextAnalysis {
                text: format!("è¿™æ˜¯ç¬¬ {} ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºæ¼”ç¤ºæ‰¹é‡å¤„ç†åŠŸèƒ½ã€‚", i),
                task: TextTask::SentimentAnalysis,
            })
            .collect();

        println!("å¤„ç† {} ä¸ªè¯·æ±‚...", batch_size);

        let start_time = std::time::Instant::now();
        let handles: Vec<_> = requests
            .into_iter()
            .map(|request| {
                let service = &self.service;
                tokio::spawn(async move { service.process_request(request).await })
            })
            .collect();

        let mut success_count = 0;
        let mut total_time = 0;

        for handle in handles {
            match handle.await {
                Ok(Ok(response)) => {
                    success_count += 1;
                    total_time += response.processing_time_ms;
                }
                Ok(Err(e)) => {
                    println!("è¯·æ±‚å¤„ç†å¤±è´¥: {}", e);
                }
                Err(e) => {
                    println!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e);
                }
            }
        }

        let total_time_elapsed = start_time.elapsed().as_millis() as u64;
        let average_time = if success_count > 0 {
            total_time / success_count
        } else {
            0
        };

        println!("æ‰¹é‡å¤„ç†å®Œæˆ:");
        println!("  - æˆåŠŸè¯·æ±‚: {}/{}", success_count, batch_size);
        println!("  - æ€»è€—æ—¶: {}ms", total_time_elapsed);
        println!("  - å¹³å‡å¤„ç†æ—¶é—´: {}ms", average_time);
        println!(
            "  - ååé‡: {:.2} req/s",
            (success_count as f64 * 1000.0) / total_time_elapsed as f64
        );

        Ok(())
    }

    /// æ¼”ç¤ºæ‰¹é‡å¤„ç†ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_batch_processing(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ‰¹é‡å¤„ç†æ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºæœåŠ¡æŒ‡æ ‡
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_service_metrics(&self) -> Result<()> {
        println!("\nğŸ“Š æ¼”ç¤ºæœåŠ¡æŒ‡æ ‡:");
        println!("================================");

        let metrics = self.service.get_metrics().await;

        println!("AI/ML æœåŠ¡æŒ‡æ ‡:");
        println!("  - æ€»è¯·æ±‚æ•°: {}", metrics.total_requests);
        println!("  - æˆåŠŸè¯·æ±‚æ•°: {}", metrics.successful_requests);
        println!("  - å¤±è´¥è¯·æ±‚æ•°: {}", metrics.failed_requests);
        println!(
            "  - å¹³å‡å“åº”æ—¶é—´: {:.2}ms",
            metrics.average_response_time_ms
        );
        println!("  - æ¨¡å‹åŠ è½½æ¬¡æ•°: {}", metrics.model_load_count);
        println!("  - ç¼“å­˜å‘½ä¸­ç‡: {:.2}%", metrics.cache_hit_rate * 100.0);

        if metrics.total_requests > 0 {
            let success_rate =
                (metrics.successful_requests as f64 / metrics.total_requests as f64) * 100.0;
            println!("  - æˆåŠŸç‡: {:.2}%", success_rate);
        }

        Ok(())
    }

    /// æ¼”ç¤ºæœåŠ¡æŒ‡æ ‡ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_service_metrics(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æœåŠ¡æŒ‡æ ‡æ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºæ€§èƒ½åŸºå‡†æµ‹è¯•
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_performance_benchmark(&self) -> Result<()> {
        println!("\nâš¡ æ¼”ç¤ºæ€§èƒ½åŸºå‡†æµ‹è¯•:");
        println!("================================");

        let iterations = 100;
        let mut total_time = 0;
        let mut success_count = 0;

        println!("æ‰§è¡Œ {} æ¬¡æƒ…æ„Ÿåˆ†æè¯·æ±‚...", iterations);

        for i in 1..=iterations {
            let request = AIMLRequest::TextAnalysis {
                text: format!("åŸºå‡†æµ‹è¯•æ–‡æœ¬ #{}", i),
                task: TextTask::SentimentAnalysis,
            };

            let start = std::time::Instant::now();
            match self.service.process_request(request).await {
                Ok(response) => {
                    let duration = start.elapsed().as_millis() as u64;
                    total_time += duration;
                    success_count += 1;

                    if i % 20 == 0 {
                        println!("  å·²å®Œæˆ {}/{} è¯·æ±‚", i, iterations);
                    }
                }
                Err(e) => {
                    println!("  è¯·æ±‚ {} å¤±è´¥: {}", i, e);
                }
            }
        }

        let average_time = if success_count > 0 {
            total_time / success_count
        } else {
            0
        };
        let throughput = if total_time > 0 {
            (success_count as f64 * 1000.0) / total_time as f64
        } else {
            0.0
        };

        println!("\nåŸºå‡†æµ‹è¯•ç»“æœ:");
        println!("  - æ€»è¯·æ±‚æ•°: {}", iterations);
        println!("  - æˆåŠŸè¯·æ±‚æ•°: {}", success_count);
        println!(
            "  - æˆåŠŸç‡: {:.2}%",
            (success_count as f64 / iterations as f64) * 100.0
        );
        println!("  - å¹³å‡å“åº”æ—¶é—´: {}ms", average_time);
        println!("  - ååé‡: {:.2} req/s", throughput);

        Ok(())
    }

    /// æ¼”ç¤ºæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_performance_benchmark(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½åŸºå‡†æµ‹è¯•æ¼”ç¤º");
        Ok(())
    }

    /// æ¼”ç¤ºé…ç½®ç®¡ç†
    #[cfg(feature = "with-ai-ml")]
    pub async fn demo_configuration_management(&self) -> Result<()> {
        println!("\nâš™ï¸  æ¼”ç¤ºé…ç½®ç®¡ç†:");
        println!("================================");

        let current_metrics = self.service.get_metrics().await;
        println!("å½“å‰é…ç½®:");
        println!("  - æ¨¡å‹ç¼“å­˜å¤§å°: {}", self.service.config.model_cache_size);
        println!(
            "  - æ¨ç†è¶…æ—¶æ—¶é—´: {}ms",
            self.service.config.inference_timeout_ms
        );
        println!("  - æ‰¹å¤„ç†å¤§å°: {}", self.service.config.batch_size);
        println!("  - GPU åŠ é€Ÿ: {}", self.service.config.enable_gpu);
        println!(
            "  - æ¨¡å‹æ›´æ–°é—´éš”: {}s",
            self.service.config.model_update_interval
        );

        println!("\næ€§èƒ½ä¼˜åŒ–å»ºè®®:");
        if current_metrics.average_response_time_ms > 1000.0 {
            println!("  - å¹³å‡å“åº”æ—¶é—´è¾ƒé«˜ï¼Œå»ºè®®å¯ç”¨ GPU åŠ é€Ÿ");
        }
        if current_metrics.cache_hit_rate < 0.8 {
            println!("  - ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ ç¼“å­˜å¤§å°");
        }
        if current_metrics.total_requests > 1000 {
            println!("  - è¯·æ±‚é‡è¾ƒå¤§ï¼Œå»ºè®®å¢åŠ æ‰¹å¤„ç†å¤§å°");
        }

        Ok(())
    }

    /// æ¼”ç¤ºé…ç½®ç®¡ç†ï¼ˆé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼‰
    #[cfg(not(feature = "with-ai-ml"))]
    pub async fn demo_configuration_management(&self) -> Result<()> {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡é…ç½®ç®¡ç†æ¼”ç¤º");
        Ok(())
    }
}

/// ä¸»å‡½æ•°æ¼”ç¤ºé«˜çº§ AI/ML åŠŸèƒ½
#[tokio::main]
async fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt::init();

    println!("ğŸ¤– é«˜çº§ AI/ML åŠŸèƒ½æ¼”ç¤º");
    println!("================================");

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† AI/ML åŠŸèƒ½
    #[cfg(not(feature = "with-ai-ml"))]
    {
        println!("âš ï¸  AI/ML åŠŸèƒ½æœªå¯ç”¨");
        println!("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯ç”¨:");
        println!("  cargo run --example advanced_ai_ml_demo --features with-ai-ml");
        return Ok(());
    }

    // åˆ›å»º AI/ML æ¼”ç¤ºç®¡ç†å™¨
    let demo_manager = AIMLDemoManager::new();

    // åˆå§‹åŒ– AI/ML æœåŠ¡
    #[cfg(feature = "with-ai-ml")]
    demo_manager.initialize().await?;

    // æ¼”ç¤ºæ–‡æœ¬åˆ†æåŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    demo_manager.demo_text_analysis().await?;

    // æ¼”ç¤ºå›¾åƒåˆ†æåŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    demo_manager.demo_image_analysis().await?;

    // æ¼”ç¤ºæ¨èç³»ç»ŸåŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    demo_manager.demo_recommendation_system().await?;

    // æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½
    #[cfg(feature = "with-ai-ml")]
    demo_manager.demo_anomaly_detection().await?;

    // æ¼”ç¤ºæ‰¹é‡å¤„ç†
    #[cfg(feature = "with-ai-ml")]
    demo_manager.demo_batch_processing().await?;

    // æ¼”ç¤ºæ€§èƒ½åŸºå‡†æµ‹è¯•
    #[cfg(feature = "with-ai-ml")]
    demo_manager.demo_performance_benchmark().await?;

    // æ¼”ç¤ºæœåŠ¡æŒ‡æ ‡
    demo_manager.demo_service_metrics().await?;

    // æ¼”ç¤ºé…ç½®ç®¡ç†
    demo_manager.demo_configuration_management().await?;

    println!("\nâœ… é«˜çº§ AI/ML åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼");
    println!();
    println!("ğŸ¯ ä¸»è¦ç‰¹æ€§:");
    println!("- å¤šæ¨¡æ€ AI æœåŠ¡ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ï¼‰");
    println!("- æ™ºèƒ½æ¨èç³»ç»Ÿ");
    println!("- å¼‚å¸¸æ£€æµ‹å’Œç›‘æ§");
    println!("- æ¨¡å‹ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶");
    println!("- å®æ—¶æ¨ç†æœåŠ¡");
    println!("- æ‰¹é‡å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–");
    println!("- å®Œæ•´çš„æŒ‡æ ‡ç›‘æ§");
    println!("- çµæ´»çš„é…ç½®ç®¡ç†");
    println!();
    println!("ğŸ“š æ”¯æŒçš„ AI æ¡†æ¶:");
    println!("- Candle: Rust åŸç”Ÿæ·±åº¦å­¦ä¹ æ¡†æ¶");
    println!("- ONNX Runtime: è·¨å¹³å°æ¨ç†å¼•æ“");
    println!("- PyTorch: é€šè¿‡ tch ç»‘å®š");
    println!("- Tokenizers: æ–‡æœ¬å¤„ç†å·¥å…·");

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    #[cfg(feature = "with-ai-ml")]
    async fn test_ai_ml_demo_manager() {
        let demo_manager = AIMLDemoManager::new();
        let metrics = demo_manager.service.get_metrics().await;
        assert_eq!(metrics.total_requests, 0);
    }

    #[tokio::test]
    #[cfg(not(feature = "with-ai-ml"))]
    async fn test_ai_ml_demo_manager_no_feature() {
        let demo_manager = AIMLDemoManager::new();
        // å¯¹äºé AI/ML ç‰¹æ€§ç‰ˆæœ¬ï¼Œåªæµ‹è¯•ç»“æ„ä½“åˆ›å»º
        assert!(true); // ç®€å•çš„æ–­è¨€ï¼Œç¡®ä¿ç»“æ„ä½“å¯ä»¥åˆ›å»º
    }

    #[cfg(feature = "with-ai-ml")]
    #[tokio::test]
    async fn test_text_analysis() {
        let demo_manager = AIMLDemoManager::new();
        demo_manager.initialize().await.unwrap();

        let request = AIMLRequest::TextAnalysis {
            text: "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬".to_string(),
            task: TextTask::SentimentAnalysis,
        };

        let response = demo_manager.service.process_request(request).await.unwrap();
        assert!(response.confidence > 0.0);
    }
}
